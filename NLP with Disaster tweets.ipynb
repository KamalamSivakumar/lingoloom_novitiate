{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ccd0d48-c9c7-4d4f-9fda-42b56852df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3c9ce92a-2599-488a-a762-78e07258fd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Objective: To understand the usage of nltk and spacy libraries with respect to categorising tweets as disaster or not.\n",
    "#Data used: From Kaggle competitions \"getting started with NLP\", \"NLP with Disaster tweets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "08806ae6-7369-444a-887d-e1f6ecb4e445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv(r\"C:\\Users\\kamalam.s\\Desktop\\kamalam's\\nlp dev\\data\\disaster tweets\\train.csv\")\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0ae27bd5-bc7f-435a-b706-1c3178004c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7613"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2190e55e-87aa-416f-b97c-df2098c1237c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "tweets['spacy objects'] = tweets['text'].apply(nlp)\n",
    "\n",
    "for doc in tweets['spacy objects']:\n",
    "    x = doc[0].text\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "338e617d-924f-4d5e-90b2-701c3859588b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               3271\n",
       "keyword          3229\n",
       "location         2196\n",
       "text             3271\n",
       "target           3271\n",
       "spacy objects    3271\n",
       "dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Target Distribution\n",
    "tweets[tweets['target']==1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "41c0871b-c2de-41eb-af28-c83d22adcc0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               4342\n",
       "keyword          4323\n",
       "location         2884\n",
       "text             4342\n",
       "target           4342\n",
       "spacy objects    4342\n",
       "dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[tweets['target']==0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dc050ada-0167-430b-be97-f95b35471ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#pre-processing the texts\n",
    "def preprocess_text(text):\n",
    "    #remove special characters\n",
    "    text = re.sub(r'@\\w+', '', text) #removes @kamalam\n",
    "    text = re.sub(r'#\\w+', '', text) #removes #kamalam\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) #removes any character other than alphabets and whitspace\n",
    "    text = re.sub(r'http\\S+', '', text) #removes \"https://kamalamsivakumar.com\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7b5f3d04-b18d-4c96-bff1-fadda364fab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample preprocess\n",
    "preprocess_text(\"https://www.kaggle.com/code/zinebelhouz/nlp-with-disaster-tweets-beginner-friendly/notebook#RNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7402d78e-fa04-4733-9ac6-026fa11b03c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK based pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4428c4c9-53d4-4be7-9896-56489d97f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8755df0b-dd27-407c-b251-6b23abe9118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp preprocessing\n",
    "def nlp_preprocess_stem(text):\n",
    "    text = preprocess_text(text) #preliminary preprocessing\n",
    "    text = text.lower()\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    processed_text = ' '.join(tokens)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "48fe96e7-006e-4a66-96ef-718d5f343f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'earthquak url'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_preprocess_stem('@kartikmention is in an earthquake,#hastag URL : https://www.kaggle.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7b40b0ed-3189-41c3-8c8e-d298eb2d8210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_preprocess_lemma(text):\n",
    "    text = preprocess_text(text) #preliminary preprocessing\n",
    "    text = text.lower()\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [re.sub(r'[^a-zA-Z0-9]', '', token) for token in tokens if token.strip()]\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    pos_tags = []\n",
    "    for token in tokens:\n",
    "        if token:\n",
    "            pos_tags.append(nltk.pos_tag([token])[0])\n",
    "    \n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "12159d8e-c7b5-4850-962a-5bd2ba625002",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = nlp_preprocess_lemma('@kartikmention is in an earthquake,#hastag URL : https://www.kaggle.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ece8a922-372e-40b5-9dbe-6ae345ceee29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('earthquake', 'NN'), ('url', 'NN')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "823b174d-bd17-4c6d-8ec3-c1d621536f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = []\n",
    "for token, pos in ans[0:]:\n",
    "    pos = pos[0].lower()  \n",
    "    pos = pos if pos in ['a', 's', 'r', 'n', 'v'] else 'n' \n",
    "    lemmatized_token = lemmatizer.lemmatize(token, pos=pos)\n",
    "    lemmatized_tokens.append(lemmatized_token)\n",
    "\n",
    "# Join tokens back into a single string\n",
    "processed_text = ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bdb2adf1-245e-47ba-97f4-51b358543072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'earthquake url'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "108bb22c-c913-4b85-a382-3e647092fd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spaCy based preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "82209036-94ab-4247-9841-19f285a8fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def spacy_preprocess(text):\n",
    "    text = preprocess_text(text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    doc = nlp(text)\n",
    "\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8ff06117-2968-4f54-afbe-e076424e8e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  earthquake url  '"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_preprocess('@kartikmention is in an earthquake,#hastag URL : https://www.kaggle.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "95bb3861-c974-4965-97bf-80f9aabb33a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>spacy objects</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>(Our, Deeds, are, the, Reason, of, this, #, ea...</td>\n",
       "      <td>deed reason   allah forgive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>(Forest, fire, near, La, Ronge, Sask, ., Canada)</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>(All, residents, asked, to, ', shelter, in, pl...</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>(13,000, people, receive, #, wildfires, evacua...</td>\n",
       "      <td>people receive   evacuation order california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>(Just, got, sent, this, photo, from, Ruby, #, ...</td>\n",
       "      <td>get send photo ruby   smoke   pour school</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                      spacy objects  \\\n",
       "0       1  (Our, Deeds, are, the, Reason, of, this, #, ea...   \n",
       "1       1   (Forest, fire, near, La, Ronge, Sask, ., Canada)   \n",
       "2       1  (All, residents, asked, to, ', shelter, in, pl...   \n",
       "3       1  (13,000, people, receive, #, wildfires, evacua...   \n",
       "4       1  (Just, got, sent, this, photo, from, Ruby, #, ...   \n",
       "\n",
       "                                      processed_text  \n",
       "0                        deed reason   allah forgive  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  resident ask shelter place notify officer evac...  \n",
       "3       people receive   evacuation order california  \n",
       "4          get send photo ruby   smoke   pour school  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['processed_text'] = [spacy_preprocess(text) for text in tweets['text']]\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bb77cb7d-6daf-46df-a66b-281f25881d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>hear   different city stay safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>forest fire spot pond geese flee street save</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>apocalypse light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>typhoon soudelor kill   china taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash   \n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...   \n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "                                 processed_text  \n",
       "0                     happen terrible car crash  \n",
       "1               hear   different city stay safe  \n",
       "2  forest fire spot pond geese flee street save  \n",
       "3                            apocalypse light    \n",
       "4          typhoon soudelor kill   china taiwan  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_test = pd.read_csv(r\"C:\\Users\\kamalam.s\\Desktop\\kamalam's\\nlp dev\\data\\disaster tweets\\test.csv\")\n",
    "tweets_test['processed_text'] = [spacy_preprocess(text) for text in tweets_test['text']]\n",
    "tweets_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4200c7d9-d22b-425c-bca1-9a3834d75e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tweets['target']\n",
    "X_train = tweets['processed_text']\n",
    "X_test = tweets_test['processed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "41bd970b-5724-4e14-a138-97a8e8d243d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             deed reason   allah forgive\n",
       "1                   forest fire near la ronge sask canada\n",
       "2       resident ask shelter place notify officer evac...\n",
       "3            people receive   evacuation order california\n",
       "4               get send photo ruby   smoke   pour school\n",
       "                              ...                        \n",
       "7608         giant crane hold bridge collapse nearby home\n",
       "7609       control wild fire california northern state...\n",
       "7610                           m   utckm s volcano hawaii\n",
       "7611    police investigate ebike collide car little po...\n",
       "7612    late home raze northern california wildfire   ...\n",
       "Name: processed_text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ca7cfaf-2f15-4389-88e4-f3446d3634ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial NB classification\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "nb = MultinomialNB(alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a3d3286-5de0-448d-98a5-4f38cfd310c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer() #returns tfidf matrix for a raw document/text\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab58cbae-e2da-46c2-a39c-d3e064e84268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tfidf shape: (7613, 11092)\n",
      "X_train shape: (7613,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train_tfidf shape:\", X_train_tfidf.shape)\n",
    "print(\"X_train shape:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fd713fe-6d5e-43a2-ad4a-1c20c0706280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model for Fold 1\n",
      "Fold1 F1-score : 0.7421686746987952\n",
      "Training Model for Fold 2\n",
      "Fold2 F1-score : 0.7531847133757962\n",
      "Training Model for Fold 3\n",
      "Fold3 F1-score : 0.7240829346092504\n",
      "Training Model for Fold 4\n",
      "Fold4 F1-score : 0.7256347256347256\n",
      "Training Model for Fold 5\n",
      "Fold5 F1-score : 0.7389558232931727\n",
      "\n",
      "Average F1-score across all folds: 0.7368053743223479\n"
     ]
    }
   ],
   "source": [
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "f1_scores = []\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(stratified_kfold.split(X_train_tfidf, y_train)):\n",
    "    print(f'Training Model for Fold {fold+1}')\n",
    "\n",
    "    X_fold_train, X_fold_test = X_train_tfidf[train_index], X_train_tfidf[test_index]\n",
    "    y_fold_train, y_fold_test = y_train.iloc[train_index] , y_train.iloc[test_index]\n",
    "\n",
    "    nb.fit(X_fold_train, y_fold_train)\n",
    "    predictions = nb.predict(X_fold_test)\n",
    "\n",
    "    f1 = f1_score(y_fold_test, predictions)\n",
    "    f1_scores.append(f1)\n",
    "    print(f'Fold{fold+1} F1-score : {f1}')\n",
    "\n",
    "print()\n",
    "print(f\"Average F1-score across all folds: {np.mean(f1_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbbe0890-467c-48ad-9d22-ca652e8db8ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb.fit(X_train_tfidf[:, :6748], y_train)\n",
    "y_test = nb.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1341dd47-b6d4-4e59-93b1-cc80175fae79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#include id in the test dataset\n",
    "for i in range(len(y_test)):\n",
    "    print(i,\"\\t\", y_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "afcab838-b418-4480-bc4a-fadd0fbdef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a pre-trained model for text classification\n",
    "#Fine-Tuning BERT for text classification\n",
    "import transformers\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig,BertTokenizer,get_linear_schedule_with_warmup\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d3e569ce-2016-4eee-a3a2-430a719f9a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = X_train\n",
    "labels = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c3fda68f-91db-40fd-a77a-1a7e97c0c070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  deed reason   allah forgive\n",
      "Tokenized:  ['deed', 'reason', 'allah', 'forgive']\n",
      "Token IDs:  [15046, 3114, 16455, 9641]\n"
     ]
    }
   ],
   "source": [
    "print(' Original: ', tweet[0])\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(tweet[0]))\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweet[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "31431f86-2f62-4fb9-ab2b-8e7dccacd8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  40\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for sent in tweet:\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1077defb-4f8f-440e-a6e7-c86df97a8bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  deed reason   allah forgive\n",
      "Token IDs: tensor([  101, 15046,  3114, 16455,  9641,   102,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "for tweets in tweet:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        tweets,                     # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = max_len,           \n",
    "                        pad_to_max_length = True,  # Pad & truncate all sentences.\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pyTorch tensors. \"tf\" for tensorflow tensors\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', tweet[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6828da26-e66a-454c-aeb5-e856b920c002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6,090 training samples\n",
      "1,523 validation samples\n"
     ]
    }
   ],
   "source": [
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "#80%-20% train-validation split.\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.8 * len(dataset))\n",
    "#val_size = int(0.2 * len(dataset))\n",
    "val_size = len(dataset)  - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "27d51847-1a58-4bc8-88d4-0e802b8e3b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size for training\n",
    "batch_size = 32\n",
    " \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d13d752c-4472-4a4c-a403-51ad5c17583b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f5c88c34-c48c-4093-8469-d31872206f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b045b02e-258c-44da-9364-f04044dfa3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e63c3ad1-f895-4a10-981d-5289599d56ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, \n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "19f98076-d641-4b08-9e47-9c6deb17e2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ede49be0-17ad-44b4-b070-6ee03aa10fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    #Takes a time in seconds and returns a string hh:mm:ss\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "deae895b-2c9e-4cd9-bf59-0749244b1317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:50:21\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.83\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:36:39\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.83\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epoch took: 0:34:21\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.83\n",
      "\n",
      "Training complete!\n",
      "Total training took 2:11:32 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import datetime\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "training_stats = []\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    # Perform one full pass over the training set.\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)        \n",
    "        loss = output.loss\n",
    "        total_train_loss += loss.item()\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "    #Validation Set\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    best_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        with torch.no_grad():        \n",
    "            output= model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "        loss = output.loss\n",
    "        total_eval_loss += loss.item()\n",
    "        logits = output.logits\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        # Calculate the accuracy for this batch of test sentences, and accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    if avg_val_accuracy > best_eval_accuracy:\n",
    "        torch.save(model, 'bert_model')\n",
    "        best_eval_accuracy = avg_val_accuracy\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9c57f793-6893-4bce-b47c-a255ab4b2453",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('bert_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d7baf9f2-0189-4c31-9dd3-7b2e46b6335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids = []\n",
    "test_attention_masks = []\n",
    "for tweet in X_test:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        tweet,                     \n",
    "                        add_special_tokens = True, \n",
    "                        max_length = max_len,           \n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',\n",
    "                   )\n",
    "    test_input_ids.append(encoded_dict['input_ids'])\n",
    "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
    "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
    "test_attention_masks = torch.cat(test_attention_masks, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8ae33446-71eb-45cc-b28b-dd66e86211e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks)\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fb154212-19c8-4932-9081-282bde3e3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for batch in test_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        with torch.no_grad():        \n",
    "            output= model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask)\n",
    "            logits = output.logits\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "            \n",
    "            predictions.extend(list(pred_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a2e8fee9-a3b4-44ae-87c3-2245eb9bca6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       1\n",
       "1         2       1\n",
       "2         3       1\n",
       "3         9       0\n",
       "4        11       1\n",
       "...     ...     ...\n",
       "3258  10861       0\n",
       "3259  10865       1\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       1\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output = pd.DataFrame()\n",
    "df_output['id'] = tweets_test['id']\n",
    "df_output['target'] = predictions\n",
    "df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de4126e-ac8b-4e5d-8a0c-619386c891b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
